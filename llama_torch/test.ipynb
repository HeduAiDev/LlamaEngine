{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    RowParallelLinear,\n",
    "    VocabParallelEmbedding,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    print('x.shape', x.shape)\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = 1\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = torch.nn.Linear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wk = torch.nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wv = torch.nn.Linear(\n",
    "            args.dim,\n",
    "            self.n_kv_heads * self.head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(\n",
    "            keys, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(\n",
    "            values, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(\n",
    "            1, 2\n",
    "        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_dim, dim, bias=False, \n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            dim, hidden_dim, bias=False )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            params.vocab_size, params.dim\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(\n",
    "            params.dim, params.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "\n",
    "def tensor_simple_check(tensor):\n",
    "    size = tensor.numel()\n",
    "    mean = int(tensor.mean().item() * 100000)\n",
    "    var = int(tensor.var().item() * 100000)\n",
    "    sum_ = int(tensor.sum().item() * 100000)\n",
    "    max_ = int(tensor.max().item() * 100000)\n",
    "    min_ = int(tensor.min().item() * 100000)\n",
    "    return f\"size:{size} mean:{mean} var:{var} sum:{sum_} max:{max_} min:{min_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9014,  0.4814,  1.1982,  ...,  1.8369, -1.2959, -0.9722],\n",
       "         [-0.7441,  1.8018,  0.2996,  ...,  1.3906, -1.1973, -0.1996],\n",
       "         [ 0.0576,  1.1680, -1.8213,  ..., -0.0284, -0.1205, -0.8081],\n",
       "         ...,\n",
       "         [-0.3916, -0.3425,  0.7056,  ...,  1.3506, -1.4277,  0.0506],\n",
       "         [ 1.0928,  0.9429,  1.0674,  ..., -0.1113, -0.6655,  1.1621],\n",
       "         [-1.5439,  2.0352,  0.4126,  ...,  0.4929,  0.7021,  0.2098]],\n",
       "\n",
       "        [[-0.9614, -0.0999, -1.0635,  ..., -0.0920, -0.2527, -0.2391],\n",
       "         [-0.6328, -0.6167, -0.2546,  ...,  1.0791,  1.0078, -1.1973],\n",
       "         [-0.7261, -0.9453, -1.3281,  ..., -1.8027,  0.5903,  0.6675],\n",
       "         ...,\n",
       "         [ 1.6670, -1.4160,  0.0227,  ..., -0.6963,  0.5195,  0.7085],\n",
       "         [-2.5078, -0.0887,  1.4941,  ..., -1.1475, -1.4961,  1.8877],\n",
       "         [-1.8164, -0.5537, -1.8857,  ...,  1.3623,  1.0166,  0.8076]],\n",
       "\n",
       "        [[-0.3293, -0.4617,  2.1035,  ...,  1.5527,  0.5000,  1.5967],\n",
       "         [-1.6006, -0.2942, -1.1025,  ..., -0.9551,  0.1294, -0.4775],\n",
       "         [ 1.6182, -0.2148, -0.7891,  ..., -0.5459, -0.9341,  1.7900],\n",
       "         ...,\n",
       "         [-0.4583, -0.5654,  0.6528,  ...,  1.1885, -0.0586, -2.1797],\n",
       "         [ 0.0272, -1.6328,  0.2418,  ..., -0.2942, -1.6807,  0.1445],\n",
       "         [ 1.2051,  0.9922, -2.8652,  ..., -1.0264,  1.5674,  0.6191]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'size:46080 mean:-1049 var:100292 sum:-48300000 max:388671 min:-390039'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([3, 20, 384])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'size:46080 mean:-1011 var:100292 sum:-46650000 max:391015 min:-392773'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9014,  0.4814,  1.1982,  ...,  1.8369, -1.2959, -0.9722],\n",
       "         [-1.9180,  0.3474, -0.6460,  ...,  1.3906, -1.1973, -0.1996],\n",
       "         [-1.0859, -0.4336,  0.7290,  ..., -0.0284, -0.1205, -0.8081],\n",
       "         ...,\n",
       "         [-0.2216,  0.4707, -0.2837,  ...,  1.3506, -1.4277,  0.0505],\n",
       "         [ 1.4297, -0.1981, -0.0931,  ..., -0.1113, -0.6655,  1.1621],\n",
       "         [-1.8311,  1.7812,  0.7656,  ...,  0.4929,  0.7021,  0.2098]],\n",
       "\n",
       "        [[-0.9614, -0.0999, -1.0635,  ..., -0.0920, -0.2527, -0.2391],\n",
       "         [ 0.1770, -0.8657, -1.6631,  ...,  1.0791,  1.0078, -1.1973],\n",
       "         [ 1.1621, -0.2668,  0.5249,  ..., -1.8027,  0.5903,  0.6675],\n",
       "         ...,\n",
       "         [-1.8203, -1.2129,  0.2583,  ..., -0.6963,  0.5195,  0.7085],\n",
       "         [-1.7227,  1.8252,  0.0882,  ..., -1.1475, -1.4961,  1.8877],\n",
       "         [-1.7129, -0.8198, -2.2012,  ...,  1.3623,  1.0166,  0.8076]],\n",
       "\n",
       "        [[-0.3293, -0.4617,  2.1035,  ...,  1.5527,  0.5000,  1.5967],\n",
       "         [-0.6172, -1.5059, -0.8228,  ..., -0.9551,  0.1294, -0.4775],\n",
       "         [-0.4780,  1.5605, -1.4688,  ..., -0.5459, -0.9341,  1.7900],\n",
       "         ...,\n",
       "         [-0.4175,  0.5962, -0.2588,  ...,  1.1885, -0.0585, -2.1797],\n",
       "         [-1.2080, -1.0986,  0.9722,  ..., -0.2942, -1.6807,  0.1444],\n",
       "         [ 1.0430,  1.1611, -2.7832,  ..., -1.0264,  1.5674,  0.6191]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "input = torch.randn((3, 20, 768), dtype=torch.float16)\n",
    "display(input)\n",
    "display(tensor_simple_check(input))\n",
    "freqs_cis = precompute_freqs_cis(768, 30, 500000.0)\n",
    "freqs_cis = freqs_cis[:20]\n",
    "display(freqs_cis.shape)\n",
    "out, _ = apply_rotary_emb(input, input.clone(), freqs_cis)\n",
    "out = out.reshape(3, 20, 768)\n",
    "display(out.shape)\n",
    "display(tensor_simple_check(out))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=1024, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000.0, max_batch_size=32, max_seq_len=2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params: ModelArgs = ModelArgs(**{\n",
    "    \"dim\": 4096, \n",
    "    \"n_layers\": 32, \n",
    "    \"n_heads\": 32, \n",
    "    \"n_kv_heads\": 8, \n",
    "    # \"vocab_size\": 128256, \n",
    "    \"vocab_size\": 1024, \n",
    "    \"ffn_dim_multiplier\": 1.3, \n",
    "    \"multiple_of\": 1024,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"rope_theta\": 500000.0,\n",
    "    # \"use_scaled_rope\": True\n",
    "})\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(1024, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (wk): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (wv): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Transformer(params)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
